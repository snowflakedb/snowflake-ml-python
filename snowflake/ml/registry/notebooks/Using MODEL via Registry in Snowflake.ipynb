{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MODEL via Registry in Snowflake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Everything\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake-ML-Python Installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Please refer to our [landing page](https://docs.snowflake.com/en/developer-guide/snowpark-ml/index) to install `snowflake-ml-python` with the latest version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Snowpark Session\n",
    "\n",
    "To avoid exposing credentials in Github, we use a small utility `SnowflakeLoginOptions`. It allows you to score your default credentials in `~/.snowsql/config` in the following format:\n",
    "\n",
    "```\n",
    "[connections]\n",
    "accountname = <string>   # Account identifier to connect to Snowflake.\n",
    "username = <string>      # User name in the account. Optional.\n",
    "password = <string>      # User password. Optional.\n",
    "dbname = <string>        # Default database. Optional.\n",
    "schemaname = <string>    # Default schema. Optional.\n",
    "warehousename = <string> # Default warehouse. Optional.\n",
    "#rolename = <string>      # Default role. Optional.\n",
    "#authenticator = <string> # Authenticator: 'snowflake', 'externalbrowser', etc\n",
    "```\n",
    "\n",
    "Please follow [this](https://docs.snowflake.com/en/user-guide/snowsql-start.html#configuring-default-connection-settings) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "session = Session.builder.configs(SnowflakeLoginOptions()).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open A Registry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we need to open a registry in a given **pre-created** database and schema, or the schema your session is actively using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTRY_DATABASE_NAME = \"MY_REGISTRY\"\n",
    "REGISTRY_SCHEMA_NAME = \"PUBLIC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import registry\n",
    "\n",
    "reg = registry.Registry(session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough Registry with a Small Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a small model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below trains a small model for demonstration purposes. The nature of the model does not matter, it is purely used to demonstrate the usage of the Registry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "target_digit = 6\n",
    "num_training_examples = 10\n",
    "svc_gamma = 0.001\n",
    "svc_C = 10.0\n",
    "\n",
    "clf = svm.SVC(gamma=svc_gamma, C=svc_C, probability=True)\n",
    "\n",
    "\n",
    "def one_vs_all(dataset, digit):\n",
    "    return [x == digit for x in dataset]\n",
    "\n",
    "\n",
    "# Train a classifier using num_training_examples and use the last 100 examples for test.\n",
    "train_features = digits.data[:num_training_examples]\n",
    "train_labels = one_vs_all(digits.target[:num_training_examples], target_digit)\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "test_features = digits.data[-100:]\n",
    "test_labels = one_vs_all(digits.target[-100:], target_digit)\n",
    "prediction = clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the model for future use, we need to log the model. We need to provide a model name and a version name, with the following API, a SQL MODEL object will be created on your behalf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"my_model\"\n",
    "version_name = \"v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = reg.log_model(clf, model_name=model_name, version_name=version_name, sample_input_data=train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After being logged, the model has already been ready to use in Snowflake with Warehouse!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_prediction = mv.run(test_features, method_name=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Remote prediction:\", remote_prediction[:10])\n",
    "\n",
    "print(\"Result comparison:\", np.array_equal(prediction, remote_prediction[\"output_feature_0\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All methods available in the original model can be run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv.list_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_prediction_proba = mv.run(test_features, method_name=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_proba = clf.predict_proba(test_features)\n",
    "\n",
    "print(\"Remote prediction:\", remote_prediction_proba[:10])\n",
    "\n",
    "print(\"Result comparison:\", np.allclose(prediction_proba, remote_prediction_proba.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the model and version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model being logged, beside using the returned object, there are other APIs for you to get the object to operate on model or model version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = reg.get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = m.version(version_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List models and versions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.list_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could set description of a model or a specific version of the model. They are backend by COMMENT feature in the SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.description = \"This is my model.\"\n",
    "print(m.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv.description = \"This is the first version of my model.\"\n",
    "print(mv.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics are a type of metadata annotation that can be associated with a version of models stored in the Registry. Metrics often take the form of scalars but we also support more complex objects such as arrays or dictionaries to represent metrics, as long as they are JSON serializable. In the examples below, we add scalars, dictionaries, and a 2-dimensional numpy array as metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "test_accuracy = metrics.accuracy_score(test_labels, prediction)\n",
    "print(\"Model test accuracy:\", test_accuracy)\n",
    "\n",
    "test_confusion_matrix = metrics.confusion_matrix(test_labels, prediction)\n",
    "print(\"Confusion matrix:\", test_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv.set_metric(metric_name=\"test_accuracy\", value=test_accuracy)\n",
    "\n",
    "mv.set_metric(metric_name=\"num_training_examples\", value=num_training_examples)\n",
    "\n",
    "mv.set_metric(metric_name=\"dataset_test\", value={\"accuracy\": test_accuracy})\n",
    "\n",
    "mv.set_metric(metric_name=\"confusion_matrix\", value=test_confusion_matrix.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv.get_metric(metric_name=\"confusion_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv.delete_metric(metric_name=\"confusion_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv.list_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could set a default version of a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.default = version_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use with Snowpark ML Modeling Model and Snowpark DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TABLE_NAME = \"KDDCUP99_DATASET\"\n",
    "\n",
    "kddcup99_data = datasets.fetch_kddcup99(as_frame=True)\n",
    "kddcup99_sp_df = session.create_dataframe(kddcup99_data.frame)\n",
    "kddcup99_sp_df.write.mode(\"overwrite\").save_as_table(DATA_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.preprocessing import one_hot_encoder, ordinal_encoder, standard_scaler\n",
    "from snowflake.ml.modeling.pipeline import pipeline\n",
    "from snowflake.ml.modeling.xgboost import xgb_classifier\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "quote_fn = lambda x: f'\"{x}\"'\n",
    "\n",
    "ONE_HOT_ENCODE_COL_NAMES = [\"protocol_type\", \"service\", \"flag\"]\n",
    "ORDINAL_ENCODE_COL_NAMES = [\"labels\"]\n",
    "STANDARD_SCALER_COL_NAMES = [\n",
    "    \"duration\",\n",
    "    \"src_bytes\",\n",
    "    \"dst_bytes\",\n",
    "    \"wrong_fragment\",\n",
    "    \"urgent\",\n",
    "    \"hot\",\n",
    "    \"num_failed_logins\",\n",
    "    \"num_compromised\",\n",
    "    \"num_root\",\n",
    "    \"num_file_creations\",\n",
    "    \"num_shells\",\n",
    "    \"num_access_files\",\n",
    "    \"num_outbound_cmds\",\n",
    "    \"count\",\n",
    "    \"srv_count\",\n",
    "    \"dst_host_count\",\n",
    "    \"dst_host_srv_count\",\n",
    "]\n",
    "\n",
    "TRAIN_SIZE_K = 0.2\n",
    "kddcup99_data = session.table(DATA_TABLE_NAME)\n",
    "kddcup99_data = kddcup99_data.with_columns(\n",
    "    list(map(quote_fn, ONE_HOT_ENCODE_COL_NAMES + ORDINAL_ENCODE_COL_NAMES)),\n",
    "    [\n",
    "        F.to_char(col_name, \"utf-8\")\n",
    "        for col_name in list(map(quote_fn, ONE_HOT_ENCODE_COL_NAMES + ORDINAL_ENCODE_COL_NAMES))\n",
    "    ],\n",
    ")\n",
    "kddcup99_sp_df_train, kddcup99_sp_df_test = tuple(\n",
    "    kddcup99_data.random_split([TRAIN_SIZE_K, 1 - TRAIN_SIZE_K], seed=2568)\n",
    ")\n",
    "\n",
    "pipe = pipeline.Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"OHEHOT\",\n",
    "            one_hot_encoder.OneHotEncoder(\n",
    "                handle_unknown=\"ignore\",\n",
    "                input_cols=list(map(quote_fn, ONE_HOT_ENCODE_COL_NAMES)),\n",
    "                output_cols=ONE_HOT_ENCODE_COL_NAMES,\n",
    "                drop_input_cols=True,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"ORDINAL\",\n",
    "            ordinal_encoder.OrdinalEncoder(\n",
    "                input_cols=list(map(quote_fn, ORDINAL_ENCODE_COL_NAMES)),\n",
    "                output_cols=['\"encoded_labels\"'],\n",
    "                drop_input_cols=True,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"STD\",\n",
    "            standard_scaler.StandardScaler(\n",
    "                input_cols=list(map(quote_fn, STANDARD_SCALER_COL_NAMES)),\n",
    "                output_cols=list(map(quote_fn, STANDARD_SCALER_COL_NAMES)),\n",
    "                drop_input_cols=True,\n",
    "            ),\n",
    "        ),\n",
    "        (\"CLASSIFIER\", xgb_classifier.XGBClassifier(label_cols=['\"encoded_labels\"'])),\n",
    "    ]\n",
    ")\n",
    "pipe.fit(kddcup99_sp_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pipeline_model\"\n",
    "version_name = \"v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = reg.log_model(pipe, model_name=model_name, version_name=version_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv.run(kddcup99_sp_df_test, method_name=\"predict\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use with customize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store GPT-2 Model components locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_DIR = \"/tmp/gpt-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(os.path.join(ARTIFACTS_DIR, \"model\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(ARTIFACTS_DIR, \"tokenizer\"), exist_ok=True)\n",
    "\n",
    "model.save_pretrained(os.path.join(ARTIFACTS_DIR, \"model\"))\n",
    "tokenizer.save_pretrained(os.path.join(ARTIFACTS_DIR, \"tokenizer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom model using GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.model import custom_model\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class GPT2Model(custom_model.CustomModel):\n",
    "    def __init__(self, context: custom_model.ModelContext) -> None:\n",
    "        super().__init__(context)\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.context.path(\"model\"))\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.context.path(\"tokenizer\"))\n",
    "\n",
    "    @custom_model.inference_api\n",
    "    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        def _generate(input_text: str) -> str:\n",
    "            input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "            output = self.model.generate(input_ids, max_length=50, do_sample=True, top_p=0.95, top_k=60)\n",
    "            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            return generated_text\n",
    "\n",
    "        res_df = pd.DataFrame({\"output\": pd.Series.apply(X[\"input\"], _generate)})\n",
    "        return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = GPT2Model(\n",
    "    custom_model.ModelContext(\n",
    "        models={},\n",
    "        artifacts={\n",
    "            \"model\": os.path.join(ARTIFACTS_DIR, \"model\"),\n",
    "            \"tokenizer\": os.path.join(ARTIFACTS_DIR, \"tokenizer\"),\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "gpt_model.predict(pd.DataFrame({\"input\": [\"Hello, are you GPT?\"]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, how to specify dependencies and model signature manually is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2_medium\"\n",
    "version_name = \"v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.model import model_signature\n",
    "\n",
    "mv = reg.log_model(\n",
    "    gpt_model,\n",
    "    model_name=model_name,\n",
    "    version_name=version_name,\n",
    "    conda_dependencies=[\"pytorch\", \"transformers\"],\n",
    "    signatures={\n",
    "        \"predict\": model_signature.ModelSignature(\n",
    "            inputs=[model_signature.FeatureSpec(name=\"input\", dtype=model_signature.DataType.STRING)],\n",
    "            outputs=[model_signature.FeatureSpec(name=\"output\", dtype=model_signature.DataType.STRING)],\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv.run(pd.DataFrame({\"input\": [\"Hello, are you GPT?\"]}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
