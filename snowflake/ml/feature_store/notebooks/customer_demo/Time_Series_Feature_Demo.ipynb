{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f029c96",
   "metadata": {},
   "source": [
    "- snowflake-ml-python version: 1.1.0\n",
    "- Feature Store PrPr version: 0.3.2\n",
    "- Updated date: 12/11/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba51119",
   "metadata": {},
   "source": [
    "## Before getting started\n",
    "\n",
    "### Watch out object name case sensitivity\n",
    "The Model Registry and Feature Store are not consistent with each other in the way they case names for databases, schemas, and other SQL objects. (Keep in mind that the objects in both APIs are Snowflake objects on the back end.) The model registry preserves the case of names for these objects, while the feature store converts names to uppercase unless you enclose them in double quotes. The way the feature store handles names is consistent with Snowflake’s identifier requirements. We are working to make this more consistent. In the meantime, we suggest using uppercase names in both APIs to ensure correct interoperation between the feature store and the model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160eb3e",
   "metadata": {},
   "source": [
    "## Time Series Features Demo\n",
    "This notebook demonstrates feature store with time series features. It includes an end-2-end ML experiment cycle: feature creation, training and inference. It also demonstrate the interoperation between Feature Store and Model Registry.\n",
    "\n",
    "It uses public NY taxi trip data to compute features. The public data can be downloaded from: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F, types as T\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark.types import TimestampType\n",
    "from snowflake.ml._internal.utils import identifier\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09ef0d",
   "metadata": {},
   "source": [
    "## Setup Snowflake connection and database\n",
    "For detailed session connection config, please follow this [tutorial](https://medium.com/snowflake/snowflakeloginoptions-an-easier-way-to-connect-using-python-2f0e726da936)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25723426",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(SnowflakeLoginOptions()).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9affb013",
   "metadata": {},
   "source": [
    "Below cell creates temporary database, schema and warehouse for this notebook. All temporary resources will be deleted at the end of this notebook. You can rename with your own name if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# database name where test data and feature store lives.\n",
    "FS_DEMO_DB = f\"FEATURE_STORE_TIME_SERIES_FEATURE_NOTEBOOK_DEMO\"\n",
    "# schema where test data lives.\n",
    "TEST_DATASET_SCHEMA = 'TEST_DATASET'\n",
    "# feature store name.\n",
    "FS_DEMO_SCHEMA = \"AWESOME_FS_TIME_SERIES_FEATURES\"\n",
    "# model registry database name.\n",
    "MR_DEMO_DB = f\"FEATURE_STORE_TIME_SERIES_FEATURE_NOTEBOOK_MR_DEMO\"\n",
    "# stages for UDF.\n",
    "FS_DEMO_STAGE = \"FEATURE_STORE_TIME_SERIES_FEATURE_NOTEBOOK_STAGE_DEMO\"\n",
    "FS_DEMO_STAGE_FULL_PATH = \\\n",
    "    f\"{FS_DEMO_DB}.{TEST_DATASET_SCHEMA}.{FS_DEMO_STAGE}\"\n",
    "# warehouse name used in this notebook.\n",
    "FS_DEMO_WH = \"FEATURE_STORE_TIME_SERIES_FEATURE_NOTEBOOK_DEMO\"\n",
    "\n",
    "session.sql(f\"DROP DATABASE IF EXISTS {FS_DEMO_DB}\").collect()\n",
    "session.sql(f\"DROP DATABASE IF EXISTS {MR_DEMO_DB}\").collect()\n",
    "session.sql(f\"CREATE DATABASE IF NOT EXISTS {FS_DEMO_DB}\").collect()\n",
    "session.sql(f\"\"\"CREATE SCHEMA IF NOT EXISTS \n",
    "    {FS_DEMO_DB}.{TEST_DATASET_SCHEMA}\"\"\").collect()\n",
    "session.sql(f\"CREATE OR REPLACE STAGE {FS_DEMO_STAGE_FULL_PATH}\").collect()\n",
    "session.sql(f\"CREATE WAREHOUSE IF NOT EXISTS {FS_DEMO_WH}\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52162799",
   "metadata": {},
   "source": [
    "## Create FeatureStore Client\n",
    "\n",
    "Let's first create a feature store client.\n",
    "\n",
    "We can pass in an existing database name, or a new database will be created upon the feature store initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=FS_DEMO_DB, \n",
    "    name=FS_DEMO_SCHEMA, \n",
    "    default_warehouse=FS_DEMO_WH,\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a67136a",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e75f08",
   "metadata": {},
   "source": [
    "Download Yellow Taxi Trip Records data (Jan. 2016) from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page if you don't have it already. Rename `PARQUET_FILE_LOCAL_PATH` with your local file path. Below code create a table with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_FILE_NAME = f\"yellow_tripdata_2016-01.parquet\"\n",
    "PARQUET_FILE_LOCAL_PATH = f\"file://~/Downloads/{PARQUET_FILE_NAME}\"\n",
    "\n",
    "def get_destination_table_name(original_file_name: str) -> str:\n",
    "    return original_file_name.split(\".\")[0].replace(\"-\", \"_\").upper()\n",
    "\n",
    "table_name = get_destination_table_name(PARQUET_FILE_NAME)\n",
    "session.file.put(PARQUET_FILE_LOCAL_PATH, session.get_session_stage())\n",
    "\n",
    "df = session.read \\\n",
    "    .parquet(f\"{session.get_session_stage()}/{PARQUET_FILE_NAME}\")\n",
    "for old_col_name in df.columns:\n",
    "    df = df.with_column_renamed(\n",
    "        old_col_name, \n",
    "        identifier.get_unescaped_names(old_col_name)\n",
    "    )\n",
    "\n",
    "full_table_name = f\"{FS_DEMO_DB}.{TEST_DATASET_SCHEMA}.{table_name}\"\n",
    "df.write.mode(\"overwrite\").save_as_table(full_table_name)\n",
    "rows_count = session.sql(\n",
    "    f\"SELECT COUNT(*) FROM {full_table_name}\").collect()[0][0]\n",
    "\n",
    "print(f\"{full_table_name} has total {rows_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfcfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = session.table(full_table_name)\n",
    "\n",
    "# source_df.TPEP_PICKUP_DATETIME.alias(\"PICKUP_TS\"),\n",
    "# source_df.TPEP_DROPOFF_DATETIME.alias(\"DROPOFF_TS\"),\n",
    "source_df = source_df.select(\n",
    "    [\n",
    "        \"TRIP_DISTANCE\", \n",
    "        \"FARE_AMOUNT\",\n",
    "        \"PASSENGER_COUNT\",\n",
    "        \"PULOCATIONID\",\n",
    "        \"DOLOCATIONID\",\n",
    "        F.cast(F.col(\"TPEP_PICKUP_DATETIME\") / 1000000, TimestampType())\n",
    "            .alias(\"PICKUP_TS\"),\n",
    "        F.cast(F.col(\"TPEP_DROPOFF_DATETIME\") / 1000000, TimestampType())\n",
    "            .alias(\"DROPOFF_TS\"),\n",
    "    ]).filter(\n",
    "        \"\"\"DROPOFF_TS >= '2016-01-01 00:00:00' \n",
    "            AND DROPOFF_TS < '2016-01-03 00:00:00'\n",
    "        \"\"\")\n",
    "source_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d052074",
   "metadata": {},
   "source": [
    "## Create and register new Entities\n",
    "Create entity by giving entity name and join keys. Then register it to feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70609920",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_pickup = Entity(name=\"TRIP_PICKUP\", join_keys=[\"PULOCATIONID\"])\n",
    "trip_dropoff = Entity(name=\"TRIP_DROPOFF\", join_keys=[\"DOLOCATIONID\"])\n",
    "fs.register_entity(trip_pickup)\n",
    "fs.register_entity(trip_dropoff)\n",
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c4393",
   "metadata": {},
   "source": [
    "## Define feature pipeline\n",
    "We will compute a few time series features in the pipeline here.\n",
    "Before we have *__value based range between__* in SQL, we will use a work around to mimic the calculation (NOTE: the work around won't be very accurate on computing the time series value due to missing gap filling functionality, but it should be enough for a demo purpose)\n",
    "\n",
    "We will define two feature groups:\n",
    "1. pickup features\n",
    "    - Mean fare amount over the past 2 and 5 hours\n",
    "2. dropoff features\n",
    "    - Count of trips over the past 2 and 5 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71729be3",
   "metadata": {},
   "source": [
    "### This is a UDF computing time window end\n",
    "We will later turn these into built in functions for feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(\n",
    "    name=\"vec_window_end\",\n",
    "    is_permanent=True,\n",
    "    stage_location=f'@{FS_DEMO_STAGE_FULL_PATH}',\n",
    "    packages=[\"numpy\", \"pandas\", \"pytimeparse\"],\n",
    "    replace=True,\n",
    "    session=session,\n",
    ")\n",
    "def vec_window_end_compute(\n",
    "    x: T.PandasSeries[datetime.datetime],\n",
    "    interval: T.PandasSeries[str],\n",
    ") -> T.PandasSeries[datetime.datetime]:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pytimeparse.timeparse import timeparse\n",
    "\n",
    "    time_slice = timeparse(interval[0])\n",
    "    if time_slice is None:\n",
    "        raise ValueError(f\"Cannot parse interval {interval[0]}\")\n",
    "    time_slot = (x - np.datetime64('1970-01-01T00:00:00')) \\\n",
    "        // np.timedelta64(1, 's') \\\n",
    "        // time_slice * time_slice + time_slice\n",
    "    return pd.to_datetime(time_slot, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73742b89",
   "metadata": {},
   "source": [
    "### Define feature pipeline logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Window\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "# NOTE: these time window calculations are approximates and are not handling time gaps\n",
    "\n",
    "def pre_aggregate_fn(df, ts_col, group_by_cols):\n",
    "    df = df.with_column(\"WINDOW_END\", \n",
    "            F.call_udf(\"vec_window_end\", F.col(ts_col), \"15m\"))\n",
    "    df = df.group_by(group_by_cols + [\"WINDOW_END\"]).agg(\n",
    "            F.sum(\"FARE_AMOUNT\").alias(\"FARE_SUM_1_HR\"),\n",
    "            F.count(\"*\").alias(\"TRIP_COUNT_1_HR\")\n",
    "         )\n",
    "    return df\n",
    "\n",
    "def pickup_features_fn(df):\n",
    "    df = pre_aggregate_fn(df, \"PICKUP_TS\", [\"PULOCATIONID\"])\n",
    "    \n",
    "    window1 = Window.partition_by(\"PULOCATIONID\") \\\n",
    "        .order_by(col(\"WINDOW_END\").desc()) \\\n",
    "        .rows_between(Window.CURRENT_ROW, 7)\n",
    "    window2 = Window.partition_by(\"PULOCATIONID\") \\\n",
    "        .order_by(col(\"WINDOW_END\").desc()) \\\n",
    "        .rows_between(Window.CURRENT_ROW, 19)\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            \"SUM_FARE_2_HR\",\n",
    "            \"COUNT_TRIP_2HR\",\n",
    "            \"SUM_FARE_5_HR\",\n",
    "            \"COUNT_TRIP_5HR\",\n",
    "        ],\n",
    "        [\n",
    "            F.sum(\"FARE_SUM_1_HR\").over(window1),\n",
    "            F.sum(\"TRIP_COUNT_1_HR\").over(window1),\n",
    "            F.sum(\"FARE_SUM_1_HR\").over(window2),\n",
    "            F.sum(\"TRIP_COUNT_1_HR\").over(window2),\n",
    "        ]\n",
    "    ).select(\n",
    "        [\n",
    "            col(\"PULOCATIONID\"),\n",
    "            col(\"WINDOW_END\").alias(\"TS\"),\n",
    "            (col(\"SUM_FARE_2_HR\") / col(\"COUNT_TRIP_2HR\"))\n",
    "                .alias(\"MEAN_FARE_2_HR\"),\n",
    "            (col(\"SUM_FARE_5_hr\") / col(\"COUNT_TRIP_5HR\"))\n",
    "                .alias(\"MEAN_FARE_5_HR\"),\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def dropoff_features_fn(df):\n",
    "    df = pre_aggregate_fn(df, \"DROPOFF_TS\", [\"DOLOCATIONID\"])\n",
    "    window1 = Window.partition_by(\"DOLOCATIONID\") \\\n",
    "        .order_by(col(\"WINDOW_END\").desc()) \\\n",
    "        .rows_between(Window.CURRENT_ROW, 7)\n",
    "    window2 = Window.partition_by(\"DOLOCATIONID\") \\\n",
    "        .order_by(col(\"WINDOW_END\").desc()) \\\n",
    "        .rows_between(Window.CURRENT_ROW, 19)\n",
    "\n",
    "    df = df.select(\n",
    "        [\n",
    "            col(\"DOLOCATIONID\"),\n",
    "            col(\"WINDOW_END\").alias(\"TS\"),\n",
    "            F.sum(\"TRIP_COUNT_1_HR\").over(window1) \\\n",
    "                .alias(\"COUNT_TRIP_2_HR\"),\n",
    "            F.sum(\"TRIP_COUNT_1_HR\").over(window2) \\\n",
    "                .alias(\"COUNT_TRIP_5_HR\"),\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "pickup_df = pickup_features_fn(source_df)\n",
    "pickup_df.show()\n",
    "\n",
    "dropoff_df = dropoff_features_fn(source_df)\n",
    "dropoff_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46fa4f",
   "metadata": {},
   "source": [
    "## Create FeatureViews and materialize\n",
    "\n",
    "Once the FeatureView construction is done, we can materialize the FeatureView to the Snowflake backend and incremental maintenance will start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345de0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# Due to a known issue on backend pipeline creation, \n",
    "# if the source data is created right before the \n",
    "# feature pipeline, there might be a chance for \n",
    "# dataloss, so sleep for 60s for now.\n",
    "# This issue will be fixed soon in upcoming patch.\n",
    "\n",
    "import time\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_fv = FeatureView(\n",
    "    name=\"TRIP_PICKUP_TIME_SERIES_FEATURES\", \n",
    "    entities=[trip_pickup], \n",
    "    feature_df=pickup_df, \n",
    "    timestamp_col=\"TS\",\n",
    "    refresh_freq=\"1 minute\", \n",
    ")\n",
    "pickup_fv = fs.register_feature_view(\n",
    "    feature_view=pickup_fv, \n",
    "    version=\"V1\", \n",
    "    block=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8960b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoff_fv = FeatureView(\n",
    "    name=\"TRIP_DROPOFF_TIME_SERIES_FEATURES\", \n",
    "    entities=[trip_dropoff], \n",
    "    feature_df=dropoff_df, \n",
    "    timestamp_col=\"TS\",\n",
    "    refresh_freq=\"1 minute\", \n",
    ")\n",
    "dropoff_fv = fs.register_feature_view(\n",
    "    feature_view=dropoff_fv, \n",
    "    version=\"V1\", \n",
    "    block=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02009c81",
   "metadata": {},
   "source": [
    "## Explore FeatureViews\n",
    "We can easily discover what are the materialized FeatureViews and the corresponding features with *__fs.list_feature_views()__*. \n",
    "\n",
    "We can also apply filters based on Entity name or FeatureView names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.list_feature_views(entity_name=\"TRIP_PICKUP\") \\\n",
    "    .select([\"NAME\", \"VERSION\", \"ENTITIES\", \"FEATURE_DESC\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302cf23",
   "metadata": {},
   "source": [
    "## Generate training data \n",
    "The training data generation will lookup __point-in-time correct__ feature values and join with the spine dataframe. Optionally, you can also exclude columns in the generated dataset by providing `exclude_columns` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spine_df = source_df.select([\n",
    "    \"PULOCATIONID\", \n",
    "    \"DOLOCATIONID\", \n",
    "    \"PICKUP_TS\", \n",
    "    \"FARE_AMOUNT\"])\n",
    "training_data = fs.generate_dataset(\n",
    "    spine_df=spine_df,\n",
    "    features=[pickup_fv, dropoff_fv],\n",
    "    materialized_table=\"yellow_tripdata_2016_01_training_data\",\n",
    "    spine_timestamp_col=\"PICKUP_TS\",\n",
    "    spine_label_cols = [\"FARE_AMOUNT\"]\n",
    ")\n",
    "\n",
    "training_data.df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20534b95",
   "metadata": {},
   "source": [
    "## Train model with Snowpark ML\n",
    "\n",
    "Now let's training a simple random forest model, and evaluate the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476befec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.linear_model import LinearRegression\n",
    "from snowflake.ml.modeling.impute import SimpleImputer\n",
    "from snowflake.ml.modeling import metrics as snowml_metrics\n",
    "from snowflake.snowpark.functions import col, unix_timestamp\n",
    "\n",
    "def train_model_using_snowpark_ml(training_data):\n",
    "    training_df = training_data.df\n",
    "    # preprocess the data\n",
    "    for col_name in [\"DOLOCATIONID\", \n",
    "                     \"PULOCATIONID\", \n",
    "                     \"COUNT_TRIP_2_HR\", \n",
    "                     \"COUNT_TRIP_5_HR\"]:\n",
    "        training_df = training_df.withColumn(col_name, col(col_name)\n",
    "                                             .cast(\"float\"))\n",
    "  \n",
    "    training_df = training_df.withColumn(\n",
    "        \"PICKUP_TS\", \n",
    "        unix_timestamp(col(\"PICKUP_TS\")))\n",
    "\n",
    "    train, test = training_df.random_split([0.8, 0.2], seed=42)\n",
    "    excluded_columns = [\"FARE_AMOUNT\", \"PICKUP_TS\"]\n",
    "    feature_columns = [col for col in training_df.columns \n",
    "                        if col not in excluded_columns]\n",
    "    label_column = \"FARE_AMOUNT\"\n",
    "\n",
    "    # Create the pipeline\n",
    "    steps = [\n",
    "        ('imputer', SimpleImputer(\n",
    "            input_cols=feature_columns, \n",
    "            output_cols=feature_columns, \n",
    "            drop_input_cols=True, \n",
    "            strategy=\"most_frequent\")), \n",
    "        ('linear_regression', LinearRegression(\n",
    "            input_cols=feature_columns, \n",
    "            label_cols=[label_column])) \n",
    "    ]   \n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.predict(test)\n",
    "\n",
    "    mse = snowml_metrics.mean_squared_error(\n",
    "        df=predictions, \n",
    "        y_true_col_names=label_column, \n",
    "        y_pred_col_names=\"OUTPUT_\" + label_column\n",
    "    )\n",
    "\n",
    "    r2 = snowml_metrics.r2_score(\n",
    "        df=predictions, \n",
    "        y_true_col_name=label_column, \n",
    "        y_pred_col_name=\"OUTPUT_\" + label_column\n",
    "    )\n",
    "\n",
    "    # Display the metrics\n",
    "    print(f\"Mean squared error: {mse}, R² score: {r2}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "estimator = train_model_using_snowpark_ml(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceef9e3",
   "metadata": {},
   "source": [
    "## [Predict Option 1] With local model\n",
    "Now let's predict with the model and the feature values retrieved from feature store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = training_data.df.sample(0.01).select(\n",
    "    ['PULOCATIONID', 'DOLOCATIONID', 'PICKUP_TS'])\n",
    "\n",
    "enriched_df = fs.retrieve_feature_values(\n",
    "    spine_df=pred_df, \n",
    "    features=training_data.load_features(), \n",
    "    spine_timestamp_col='PICKUP_TS'\n",
    ").drop(['PICKUP_TS']).to_pandas()\n",
    "\n",
    "pred = estimator.predict(enriched_df)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142c25c",
   "metadata": {},
   "source": [
    "## [Predict Option 2] With Model Registry\n",
    "### Step 1 : Log the model along with its training dataset metadata into Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import model_registry\n",
    "\n",
    "registry = model_registry.ModelRegistry(\n",
    "    session=session, \n",
    "    database_name=MR_DEMO_DB, \n",
    "    create_if_not_exists=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84daec8a",
   "metadata": {},
   "source": [
    "Register the dataset into model registry with `log_artifact`. Artifact is a generalized concept of ML pipeline outputs that are needed for subsequent execution. Refer to https://docs.snowflake.com/LIMITEDACCESS/snowflake-ml-model-registry for more details about the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e416eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"MY_DATASET\"\n",
    "DATASET_VERSION = \"V1\"\n",
    "\n",
    "my_dataset = registry.log_artifact(\n",
    "    artifact=training_data,\n",
    "    name=DATASET_NAME,\n",
    "    version=DATASET_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf6948",
   "metadata": {},
   "source": [
    "Now you can log the model together with the registered artifact (which is a dataset here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MY_MODEL\"\n",
    "\n",
    "model_ref = registry.log_model(\n",
    "    model_name=model_name,\n",
    "    model_version=\"V1\",\n",
    "    model=estimator,\n",
    "    artifacts=[my_dataset],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0581d",
   "metadata": {},
   "source": [
    "### Step 2 : Restore model and predict with features\n",
    "Retrieve the training dataset from registry and construct dataframe of latest feature values. Then we restore the model from registry. Finally, we can predict with latest feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich source prediction data with features\n",
    "from snowflake.ml.dataset.dataset import Dataset\n",
    "\n",
    "registered_dataset = registry.get_artifact(\n",
    "    DATASET_NAME, \n",
    "    DATASET_VERSION)\n",
    "\n",
    "enriched_df = fs.retrieve_feature_values(\n",
    "    spine_df=pred_df, \n",
    "    features=registered_dataset.load_features(), \n",
    "    spine_timestamp_col='PICKUP_TS'\n",
    ").drop(['PICKUP_TS']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd545ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = model_registry.ModelReference(\n",
    "    registry=registry, \n",
    "    model_name=model_name, \n",
    "    model_version=\"V1\"\n",
    ").load_model()\n",
    "\n",
    "pred = model_ref.predict(enriched_df)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d338c76",
   "metadata": {},
   "source": [
    "## Cleanup notebook\n",
    "Cleanup resources created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceeecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(f\"DROP DATABASE IF EXISTS {FS_DEMO_DB}\").collect()\n",
    "session.sql(f\"DROP DATABASE IF EXISTS {MR_DEMO_DB}\").collect()\n",
    "session.sql(f\"DROP WAREHOUSE IF EXISTS {FS_DEMO_WH}\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
