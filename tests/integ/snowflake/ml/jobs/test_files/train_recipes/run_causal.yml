---
# https://github.com/snowflakedb/ArcticTraining/blob/main/projects/causal/run-causal.yml
type: causal
micro_batch_size: 1
exit_iteration: 10
min_iterations: 10
train_log_metrics_path: /tmp/train_log_metrics_path.json

deepspeed:
  zero_optimization:
    stage: 3

optimizer:
  learning_rate: 1e-5

model:
  # type: "liger"
  name_or_path: hf-internal-testing/tiny-random-LlamaForCausalLM
  # name_or_path: TinyLlama/TinyLlama_v1.1
  # name_or_path: meta-llama/Llama-3.1-8B

  # attn_implementation: flash_attention_2
  attn_implementation: sdpa

  dtype: bf16

data:
  sources:
    - type: huggingface_causal
      # the first dataset is tiny but fast to download to try it out
      name_or_path: stas/gutenberg-100:train[:100]
      # this is 14GB-large
      # name_or_path: manu/project_gutenberg:en[:100]
      # split: en
      # sample_count: 100_000

  cache_dir: /tmp/data-cache
  num_proc: 16
  dl_num_workers: 1

  max_length: 2048

logger:
  level: WARNING
#  level: INFO

  output_dir: logs
  # file_output_ranks: [0,1]
  print_output_ranks:
    - 0
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7

checkpoint:
  - type: huggingface
    save_every_n_steps: 300
    # save_end_of_training: true
    output_dir: /tmp/ft-model
