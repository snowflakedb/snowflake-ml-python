#
# Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
#
import copy
import enum
import uuid

import inspect
import inflection
import numpy as np
import numpy.typing as npt
import pandas as pd
import random
import pytest

from typing import Any, Dict, List, Optional, Tuple, Union
from absl.testing.absltest import TestCase, main
{transform.test_snowpark_pandas_imports}
# from snowflake.ml import snowpark_pandas
from snowflake.ml.utils.connection_params import SnowflakeLoginOptions
from snowflake.snowpark import Session
# from snowflake.snowpark.modin import pandas as snowpark_pandas

_INFERENCE = "INFERENCE"
_EXPECTED = "EXPECTED"
_SCORE = "SCORE"


class DatasetType(enum.Enum):
    SNOWPARK_PANDAS = "SNOWPARK_PANDAS"
    PANDAS = "PANDAS"


@pytest.mark.pip_incompatible
class {transform.test_class_name}(TestCase):
    def setUp(self) -> None:
        """Creates Snowpark and Snowflake environments for testing."""
        self._session = Session.builder.configs(SnowflakeLoginOptions()).create()

    def tearDown(self) -> None:
        self._session.close()

    def _get_test_dataset(
            self, sklearn_obj: Optional[Any] = None, add_sample_weight_col: bool = False
            ) -> Tuple[pd.DataFrame, List[str], List[str]]:
        """ Constructs input dataset to be used in the integration test.

        Args:
            sklearn_obj: SKLearn object under tests. If the sklearn_obj supports multioutput, then this method will
            add extra lable columns to test multioutput functionality.
            add_sample_weight_col: If true and addiptional column named "SAMPLE_WEIGHT" will be added to the dataset
            representing the weight of each sample.

        Returns:
            A tuple containing pandas dataframe, list of input columns names, and list of lable column names.
        """
        input_df_pandas = {transform.test_dataset_func}(as_frame=True).frame

        # Some of the estimators inherit from MultiOutputMixin class but don't actually support multi task learning.
        # Those estimators can be identified by calling _is_multitask() method or checking "multioutput" tag.
        assert sklearn_obj is not None
        if (
            {transform._is_multioutput}
            and (
                (callable(getattr(sklearn_obj, "_is_multitask", None)) and sklearn_obj._is_multitask())
                or (
                    callable(getattr(sklearn_obj, "_more_tags", None))
                    and (
                        ("multioutput" in sklearn_obj._more_tags() and sklearn_obj._more_tags()["multioutput"])
                        or ("multioutput_only" in sklearn_obj._more_tags() and sklearn_obj._more_tags()["multioutput_only"])
                    )
                )
            )
            or {transform._is_multioutput_estimator}
            or {transform._is_chain_multioutput}
        ):
            input_df_pandas["target2"] = input_df_pandas["target"].apply(lambda x: 1 if not x % 2 else 0)

        # Normalize column names
        input_df_pandas.columns = [inflection.parameterize(c, "_").upper() for c in input_df_pandas.columns]

        if add_sample_weight_col:
            random.seed(0)
            input_df_pandas["SAMPLE_WEIGHT"] = np.array([random.randint(0, 100) for _ in range(input_df_pandas.shape[0])])

        # Predict UDF processes and returns data in random order.
        # Add INDEX column so that output can be sorted by that column
        # to compare results with local sklearn predict.
        input_df_pandas["INDEX"] = input_df_pandas.reset_index().index
        if {transform._is_positive_value_input}:
            input_df_pandas = input_df_pandas.abs()

        input_cols = [
                c for c in input_df_pandas.columns
                if not c.startswith("TARGET") and not c.startswith("SAMPLE_WEIGHT") and not c.startswith("INDEX")
        ]
        if {transform._is_single_col_input}:
            input_cols = [input_cols[0]]
        label_col = [c for c in input_df_pandas.columns if c.startswith("TARGET")]
        return (input_df_pandas, input_cols, label_col)

    # @staticmethod
    # def _compute_output(
    #         reg: Any, args: Any, dataset: Any, score_args: Any
    #     ) -> Dict[str, List[Union[npt.ArrayLike, float]]]:
    #     output: Dict[str, List[Union[npt.ArrayLike, float]]] = {{}}

    #     reg.fit(**args)

    #     inference_methods = ["transform", "predict"]
    #     output[_INFERENCE] = []

    #     if {transform._is_grid_search_cv} or {transform._is_randomized_search_cv}:
    #         inference_methods.remove("transform")  # underlying estimators have no method 'transform'
    #     if Sk{transform.original_class_name}.__name__ == "LocalOutlierFactor" and not reg.novelty:
    #         inference_methods.remove("predict")
        
    #     for m in inference_methods:
    #         if callable(getattr(reg, m, None)):
    #             res = getattr(reg, m)(dataset)
    #             # TODO(hayu): Remove the output manipulation as the results should be exactly the same as sklearn.
    #             if isinstance(res, snowpark_pandas.DataFrame) or isinstance(res, pd.DataFrame):
    #                 arr = res.to_numpy()
    #             elif isinstance(res, list):
    #                 arr = np.array(res)
    #             else:
    #                 arr = res
    #             if arr.ndim == 2 and arr.shape[1] == 1:
    #                 arr = arr.flatten()
    #             if len(arr.shape) == 3:
    #                 # VotingClassifier will return results of shape (n_classifiers, n_samples, n_classes)
    #                 # when voting = "soft" and flatten_transform = False. We can't handle unflatten transforms,
    #                 # so we ignore flatten_transform flag and flatten the results. We need flatten sklearn results
    #                 # also to compare with snowflake results.
    #                 arr = np.hstack(arr)  # type: ignore[arg-type]
    #             elif len(arr.shape) == 1:
    #                 # Sometimes sklearn returns results as 1D array of shape (n_samples,), but snowflake always returns
    #                 # response as 2D array of shape (n_samples, 1). Flatten the snowflake response to compare results.
    #                 arr = arr.flatten()
    #             output[_INFERENCE].append(arr)

    #     expected_methods = ["predict_proba", "predict_log_proba", "decision_function"]
    #     output[_EXPECTED] = []

    #     class_name = Sk{transform.original_class_name}.__name__
    #     if class_name in ["SGDClassifier", "NuSVC", "SVC"] or (class_name == "VotingClassifier" and reg.voting == "hard"):
    #         expected_methods.remove("predict_proba")
    #         expected_methods.remove("predict_log_proba")
    #     if Sk{transform.original_class_name}.__name__ == "LocalOutlierFactor" and not reg.novelty:
    #         expected_methods.remove("decision_function")

    #     for m in expected_methods:
    #         if callable(getattr(reg, m, None)):
    #             res = getattr(reg, m)(dataset)
    #             if isinstance(res, snowpark_pandas.DataFrame) or isinstance(res, pd.DataFrame):
    #                 arr = res.to_numpy()
    #             elif isinstance(res, list):
    #                 arr = np.array(res)
    #             else:
    #                 arr = res
    #             if arr.ndim == 2 and arr.shape[1] == 1:
    #                 arr = arr.flatten()
    #             if isinstance(arr, list):
    #                 # In case of multioutput estimators predict_proba, decision_function, etc., returns a list of
    #                 # ndarrays as output. We need to concatenate them to compare with snowflake output.
    #                 arr = np.concatenate(arr, axis=1)
    #             elif len(arr.shape) == 1:
    #                 # Sometimes sklearn returns results as 1D array of shape (n_samples,), but snowflake always returns
    #                 # response as 2D array of shape (n_samples, 1). Flatten the snowflake response to compare results.
    #                 arr = arr.flatten()
    #             output[_EXPECTED].append(arr)

    #     output[_SCORE] = []
    #     if callable(getattr(reg, "score", None)):
    #         score = getattr(reg, "score")(**score_args) if score_args else getattr(reg, "score")(**args)
    #         output[_SCORE].append(score)

    #     return output

    # def _fit_and_compare_results(
    #         self,
    #         training: DatasetType,
    #         inference: DatasetType,
    #         use_weighted_dataset: bool,
    # ) -> None:
    #     input_df_pandas = {transform.test_dataset_func}(as_frame=True).frame
    #     cols = [inflection.parameterize(c, "_").upper() for c in input_df_pandas.columns if not c.startswith("target")]
    #     cols_half_1, cols_half_2 = cols[:int(len(cols) / 2)], cols[int(len(cols) / 2) + 1:]

    #     reg = Sk{transform.original_class_name}({transform.test_estimator_input_args})

    #     # Special handle for label encoder: sklearn label encoder fit method only accept fit(y),
    #     # but our SnowML API would treat it as fit(X)
    #     _is_label_encoder = reg.__class__.__name__ == "LabelEncoder"

    #     input_df_pandas, input_cols, label_col = self._get_test_dataset(
    #             sklearn_obj=reg,
    #             add_sample_weight_col=use_weighted_dataset
    #     )
    #     input_df_snowpandas = snow_pd.DataFrame(input_df_pandas)

    #     pd_X, pd_y = input_df_pandas[input_cols], input_df_pandas[label_col].squeeze()
    #     snow_X, snow_y = input_df_snowpandas[input_cols], input_df_snowpandas[label_col].squeeze()
    #     pd_args = {{
    #         'X': pd_X,
    #         'y': pd_y,
    #     }}
    #     snow_args = {{
    #         'X': snow_X,
    #         'y': snow_y,
    #     }}
        
    #     # SnowML preprocessing class currently doesn't support sample weight
    #     if use_weighted_dataset and not {transform._is_preprocessing_module_obj}:
    #         pd_args['sample_weight'] = input_df_pandas["SAMPLE_WEIGHT"].squeeze()
    #         snow_args['sample_weight'] = input_df_snowpandas["SAMPLE_WEIGHT"].squeeze()

    #     pd_score_args = snow_score_args = None
    #     if callable(getattr(reg, "score", None)):
    #         pd_score_args = copy.deepcopy(pd_args)
    #         snow_score_args = copy.deepcopy(snow_args)
    #         score_argspec = inspect.getfullargspec(reg.score)
    #         # Some classes that has sample_weight argument in fit() but not in score().
    #         if use_weighted_dataset and 'sample_weight' not in score_argspec.args:
    #             del pd_score_args['sample_weight']
    #             del snow_score_args['sample_weight']

    #         # Some classes have different arg name in score: X -> X_test
    #         if "X_test" in score_argspec.args:
    #             pd_score_args['X_test'] = pd_score_args.pop('X')
    #             snow_score_args['X_test'] = snow_score_args.pop('X')

    #     if {transform._is_pls} or {transform._is_multioutput_estimator} or {transform._is_chain_multioutput}:
    #         if Sk{transform.original_class_name}.__name__ != "MultiOutputRegressor":
    #             pd_args['Y'] = pd_args.pop('y')
    #             snow_args['Y'] = snow_args.pop('y')

    #     # pandas
    #     if _is_label_encoder:
    #         pd_output = self._compute_output(reg, {{'y': input_df_pandas[label_col]}}, input_df_pandas[label_col], None)
    #     else:
    #         pd_output = self._compute_output(reg, pd_args, input_df_pandas[input_cols], pd_score_args)

    #     # snowpandas
    #     snowpark_pandas.init()

    #     # Integrate with native distributed preprocessing methods
    #     snow_reg = Sk{transform.original_class_name}({transform.test_estimator_input_args})
    #     args = snow_args if training == DatasetType.SNOWPARK_PANDAS else pd_args
    #     dataset, score_args = (
    #         (input_df_snowpandas[input_cols], snow_score_args) if inference == DatasetType.SNOWPARK_PANDAS
    #         else (input_df_pandas[input_cols], pd_score_args)
    #     )
    #     if _is_label_encoder:
    #         if training == DatasetType.SNOWPARK_PANDAS:
    #             snow_output = self._compute_output(reg, {{'X': input_df_snowpandas[label_col]}}, input_df_snowpandas[label_col], None)
    #         else:
    #             snow_output = self._compute_output(reg, {{'y': input_df_pandas[label_col]}}, input_df_pandas[label_col], None)
    #     else:
    #         snow_output = self._compute_output(snow_reg, args, dataset, score_args)

    #     for pd_arr, snow_arr in zip(pd_output[_INFERENCE], snow_output[_INFERENCE]):
    #         snow_arr = snow_arr.astype(pd_arr.dtype)  # type: ignore[union-attr]
    #         # TODO(snandamuri): HistGradientBoostingRegressor is returning different results in different envs.
    #         # Needs further debugging.
    #         if {transform._is_hist_gradient_boosting_regressor}:
    #             num_diffs = (~np.isclose(snow_arr, pd_arr)).sum()
    #             num_example = pd_arr.shape[0]
    #             assert num_diffs < 0.1 * num_example
    #         else:
    #             np.testing.assert_allclose(snow_arr, pd_arr, rtol=1.e-1, atol=1.e-2)  # type: ignore[arg-type]

    #     for pd_arr, snow_arr in zip(pd_output[_EXPECTED], snow_output[_EXPECTED]):
    #         snow_arr = snow_arr.astype(pd_arr.dtype)  # type: ignore[union-attr]
    #         np.testing.assert_allclose(snow_arr, pd_arr, rtol=1.e-1, atol=1.e-2)  # type: ignore[arg-type]

    #     for pd_output, snow_output in zip(pd_output[_SCORE], snow_output[_SCORE]):  # type: ignore[assignment]
    #         np.testing.assert_allclose(snow_output, pd_output, rtol=1.e-1, atol=1.e-2)  # type: ignore[call-overload]

    # def test_non_weighted_datasets_snow_snow(self) -> None:
    #     self._fit_and_compare_results(
    #         training=DatasetType.SNOWPARK_PANDAS,
    #         inference=DatasetType.SNOWPARK_PANDAS,
    #         use_weighted_dataset=False
    #     )

    # def test_non_weighted_datasets_snow_pd(self) -> None:
    #     self._fit_and_compare_results(
    #         training=DatasetType.SNOWPARK_PANDAS,
    #         inference=DatasetType.PANDAS,
    #         use_weighted_dataset=False
    #     )

    # def test_non_weighted_datasets_pd_snow(self) -> None:
    #     self._fit_and_compare_results(
    #         training=DatasetType.PANDAS,
    #         inference=DatasetType.SNOWPARK_PANDAS,
    #         use_weighted_dataset=False
    #     )

    # def _is_weighted_dataset_supported(self, klass: type) -> bool:
    #     is_weighted_dataset_supported = False
    #     for m in inspect.getmembers(klass):
    #         if inspect.isfunction(m[1]) and m[0] == "fit":
    #             argspec = inspect.getfullargspec(m[1])
    #             is_weighted_dataset_supported = True if "sample_weight" in argspec.args else False
    #     return is_weighted_dataset_supported

    # def test_weighted_datasets_snow_snow(self) -> None:
    #     if self._is_weighted_dataset_supported(Sk{transform.original_class_name}):
    #         self._fit_and_compare_results(
    #             training=DatasetType.SNOWPARK_PANDAS,
    #             inference=DatasetType.SNOWPARK_PANDAS,
    #             use_weighted_dataset=True
    #         )

    # def test_weighted_datasets_snow_pd(self) -> None:
    #     if self._is_weighted_dataset_supported(Sk{transform.original_class_name}):
    #         self._fit_and_compare_results(
    #             training=DatasetType.SNOWPARK_PANDAS,
    #             inference=DatasetType.PANDAS,
    #             use_weighted_dataset=True
    #         )

    # def test_weighted_datasets_pd_snow(self) -> None:
    #     if self._is_weighted_dataset_supported(Sk{transform.original_class_name}):
    #         self._fit_and_compare_results(
    #             training=DatasetType.PANDAS,
    #             inference=DatasetType.SNOWPARK_PANDAS,
    #             use_weighted_dataset=True
    #         )


if __name__ == "__main__":
    main()

