#
# Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
#
import inspect
import os
from typing import Iterable, Optional, Union, List, Any, Dict, Callable
from uuid import uuid4

import joblib
import json
import pandas as pd
import numpy as np
{transform.estimator_imports}
from sklearn.utils.metaestimators import available_if

from snowflake.ml.framework.base import BaseEstimator, BaseTransformer
from snowflake.ml.utils import telemetry
from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
from snowflake.ml._internal.utils import pkg_version_utils, identifier
from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
from snowflake.snowpark import DataFrame, Session
from snowflake.snowpark.functions import pandas_udf, sproc
from snowflake.snowpark.session import _get_active_session
from snowflake.snowpark.types import PandasSeries

_PROJECT = "ModelDevelopment"
# Derive subproject from module name by removing "sklearn"
# and converting module name from underscore to CamelCase
# e.g. sklearn.linear_model -> LinearModel.
_SUBPROJECT = "".join([s.capitalize() for s in "{transform.root_module_name}".replace("sklearn.", "").split("_")])


def _original_estimator_has_callable(attr : str) -> Callable[[Any], bool]:
    """ Checks that the original estimator has callable `attr`.

    Args:
        attr: Arrtibute to check for.

    Returns:
        A function which checks for the existance of callable `attr` on the given object.
    """
    def check(self) -> bool:
        """ Check for the existance of callable `attr` in self.

        Returns:
            True of the callable `attr` exists in self, False otherwise.
        """
        return callable(getattr(self._sklearn_object, attr, None))

    return check


def _transform_snowml_obj_to_sklearn_obj(obj: Any) -> Any:
    """Converts SnowML Estimator and Transformer objects to equivalent SKLearn objects.

    Args:
        obj: Source object that needs to be converted. Source object could of any type, example, lists, tuples, etc.

    Returns:
        An equivalent object with SnowML estimators and transforms replaced with equivalent SKLearn objects.
    """

    if isinstance(obj, list):
        # Apply transform function to each element in the list
        return list(map(_transform_snowml_obj_to_sklearn_obj, obj))
    elif isinstance(obj, tuple):
        # Apply transform function to each element in the tuple
        return tuple(map(_transform_snowml_obj_to_sklearn_obj, obj))
    elif isinstance(obj, BaseTransformer):
        # Convert SnowML object to equivalent SKLearn object
        return obj.get_sklearn_object()
    else:
        # Return all other objects as it is.
        return obj


def _validate_sklearn_args(args: Dict[str, Any], klass: type) -> Dict[str, Any]:
    """ Validate if all the keyword args are supported by current version of SKLearn/XGBoost object.

    Args:
        args: Dictionary of keyword args for the wrapper init method.
        klass: Underlying SKLearn/XGBoost class object.

    Raises:
        Raises an expception if a user specified arg is not supported by current version of sklearn/xgboost.
    """
    result = {{}}
    signature = inspect.signature(klass.__init__)
    for k, v in args.items():
        if k not in signature.parameters.keys(): # Arg is not supported.
            if (
                v[2] # Arg doesn't have default value in the signature.
                or (
                    v[0] != v[1]  # Value is not same as default.
                    and not (isinstance(v[0], float) and np.isnan(v[0]) and np.isnan(v[1]))) # both are not NANs
            ):
                raise RuntimeError(f"Arg {{k}} is not supported by current version of SKLearn/XGBoost.")
        else:
            result[k] = v[0]
    return result


class {transform.estimator_class_name}(BaseEstimator, BaseTransformer):
    r"""{transform.estimator_class_docstring}
    """

    def __init__(
        {transform.estimator_init_signature}
    ) -> None:
        super().__init__(custom_states=None)
        self.id = str(uuid4()).replace("-", "_").upper()
        {transform.estimator_args_transform_calls}
        init_args = {transform.sklearn_init_args_dict}
        cleaned_up_init_args = _validate_sklearn_args(
            args=init_args,
            klass={transform.root_module_name}.{transform.original_class_name}
        )
        self._sklearn_object = {transform.root_module_name}.{transform.original_class_name}(
            {transform.sklearn_init_arguments}
        )
        {transform.estimator_init_member_args}

    def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
        """
        Infer `self.input_cols` and `self.output_cols` if they are not set explicitly.

        Args:
            dataset: Input dataset.
        """
        if not self.input_cols:
            non_input_cols = []
            if self.label_cols:
                non_input_cols.extend(self.label_cols)
            if self.sample_weight_col:
                non_input_cols.extended(self.sample_weight_col)

            cols = [c for c in dataset.columns if c not in non_input_cols]
            self.set_input_cols(input_cols=cols)
        
        if not self.output_cols:
            cols = [identifier.concat_names(ids=['OUTPUT_', c]) for c in self.label_cols]
            self.set_output_cols(output_cols=cols)

    @telemetry.send_api_usage_telemetry(
        project=_PROJECT,
        subproject=_SUBPROJECT,
        custom_tags=dict([("autogen", True)]),
    )
    def fit(self, dataset: Union[DataFrame, pd.DataFrame]) -> "{transform.transformer_class_name}":
        """{transform.fit_docstring}

        Args:
            dataset: Input dataset.

        Returns:
            self. Fitted Transform.
        """
        self._infer_input_output_cols(dataset)
        if isinstance(dataset, pd.DataFrame):
            self._fit_pandas(dataset)
        elif isinstance(dataset, DataFrame):
            self._fit_snowpark(dataset)
        else:
            raise TypeError(
                f"Unexpected dataset type: {{type(dataset)}}."
                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
            )
        self._is_fitted = True
        return self

    def _fit_snowpark(self, dataset: DataFrame) -> None:
        session = dataset._session
        # Validate that key package version in user workspace are supported in snowflake conda channel
        pkg_version_utils.validate_pkg_versions_supported_in_snowflake_conda_channel(
            pkg_versions=[{transform.fit_sproc_deps}], session=session)

        # Specify input columns so column pruing will be enforced
        selected_cols = (
            self.input_cols + self.label_cols + [self.sample_weight_col]
            if self.sample_weight_col is not None
            else []
        )
        if len(selected_cols) > 0:
            dataset = dataset.select(selected_cols)

        # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
        query = str(dataset.queries["queries"][0])

        # Create a temp file and dump the transform to that file.
        local_transform_file_name = get_temp_file_path()
        joblib.dump(self._sklearn_object, local_transform_file_name)

        # Create temp stage to run fit.
        transform_stage_name = "SNOWML_TRANSFORM_{{safe_id}}".format(safe_id=self.id)
        stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {{transform_stage_name}};"
        SqlResultValidator(
            session=session,
            query=stage_creation_query
        ).has_dimensions(
            expected_rows=1, expected_cols=1
        ).has_value_match(
            row_idx=0,
            col_idx=0,
            expected_value=f"Stage area {{transform_stage_name}} successfully created."
        ).validate()

        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
        local_result_file_name = get_temp_file_path()
        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))

        # Put locally serialized transform on stage.
        session.file.put(local_transform_file_name, stage_transform_file_name, auto_compress=False, overwrite=True)

        fit_sproc_name = "SNOWML_FIT_{{safe_id}}".format(safe_id=self.id)
        @sproc(
                is_permanent=False,
                name=fit_sproc_name,
                packages=["snowflake-snowpark-python", {transform.fit_sproc_deps}],
                replace=True,
                session=session)
        def fit_wrapper_sproc(
                session: Session,
                sql_query: str,
                stage_transform_file_name: str,
                stage_result_file_name: str,
                input_cols: List[str],
                label_cols: List[str],
                sample_weight_col: Optional[str]
        ) -> str:
            import joblib
            import numpy as np
            import os
            import pandas
            import tempfile
            import inspect
            {transform.fit_sproc_imports}

            # Execute snowpark query and obtain the results as pandas dataframe
            # NB: this implies that the result data must fit into memory.
            df = session.sql(sql_query).to_pandas()

            local_transform_file = tempfile.NamedTemporaryFile(delete=True)
            local_transform_file_name = local_transform_file.name
            local_transform_file.close()

            session.file.get(stage_transform_file_name, local_transform_file_name)

            estimator = joblib.load(os.path.join(local_transform_file_name, os.listdir(local_transform_file_name)[0]))

            argspec = inspect.getfullargspec(estimator.fit)
            args = {{'X': df[input_cols]}}
            if label_cols:
                label_arg_name = "Y" if "Y" in argspec.args else "y"
                args[label_arg_name] = df[label_cols].squeeze()

            if sample_weight_col is not None and "sample_weight" in argspec.args:
                args['sample_weight'] = df[sample_weight_col].squeeze()

            estimator.fit(**args)

            local_result_file = tempfile.NamedTemporaryFile(delete=True)
            local_result_file_name = local_result_file.name
            local_result_file.close()

            joblib_dump_files = joblib.dump(estimator, local_result_file_name)
            session.file.put(local_result_file_name, stage_result_file_name, auto_compress = False, overwrite = True)

            # Note: you can add something like  + "|" + str(df) to the return string
            # to pass debug information to the caller.
            return str(os.path.basename(joblib_dump_files[0]))
        
        # Call fit sproc
        statement_params = telemetry.get_function_usage_statement_params(
            project=_PROJECT,
            subproject=_SUBPROJECT,
            function_name=telemetry.get_statement_params_full_func_name(
                inspect.currentframe(), self.__class__.__name__
            ),
            api_calls=[Session.call],
            custom_tags=dict([("autogen", True)]),
        )
        sproc_export_file_name = session.call(
            fit_sproc_name,
            query,
            stage_transform_file_name,
            stage_result_file_name,
            identifier.get_equivalent_identifier_in_the_response_pandas_dataframe(self.input_cols),
            identifier.get_equivalent_identifier_in_the_response_pandas_dataframe(self.label_cols),
            identifier.get_equivalent_identifier_in_the_response_pandas_dataframe(self.sample_weight_col),
            statement_params=statement_params,
        )

        if "|" in sproc_export_file_name:
            fields = sproc_export_file_name.strip().split("|")
            sproc_export_file_name = fields[0]
            if len(fields) > 1:
                print("\n".join(fields[1:]))

        session.file.get(os.path.join(stage_result_file_name, sproc_export_file_name), local_result_file_name)
        self._sklearn_object = joblib.load(os.path.join(local_result_file_name, sproc_export_file_name))

        cleanup_temp_files([local_transform_file_name, local_result_file_name])

    def _fit_pandas(self, dataset: pd.DataFrame) -> None:
        argspec = inspect.getfullargspec(self._sklearn_object.fit)
        args = {{'X': dataset[self.input_cols]}}
        if self.label_cols:
            label_arg_name = "Y" if "Y" in argspec.args else "y"
            args[label_arg_name] = dataset[self.label_cols].squeeze()

        if self.sample_weight_col is not None and "sample_weight" in argspec.args:
            args['sample_weight'] = dataset[self.sample_weight_col].squeeze()

        self._sklearn_object.fit(**args)

    def _get_pass_through_columns(self, dataset: DataFrame) -> List[str]:
        if self._drop_input_cols:
            return []
        else:
            return list(set(dataset.columns) - set(self.output_cols))

    def _batch_inference(
        self,
        dataset: DataFrame,
        inference_method: str,
        expected_output_cols_list: List[str],
        expected_output_cols_type: str = "",
    ) -> DataFrame:
        """Util method to create UDF and run batch inference.
        """
        if not self._is_fitted:
            raise RuntimeError(f"Estimator not fitted before calling {{inference_method}} method.")

        session = dataset._session
        # Validate that key package version in user workspace are supported in snowflake conda channel
        pkg_version_utils.validate_pkg_versions_supported_in_snowflake_conda_channel(
            pkg_versions=[{transform.predict_udf_deps}], session=session)

        # Register vectorized UDF for batch inference
        batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{{safe_id}}_{{method}}".format(
                safe_id=self.id, method=inference_method)

        # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
        # will try to pickle all of self which fails.
        estimator = self._sklearn_object

        # Input columns for UDF are sorted by column names.
        # We need actual order of input cols to reorder dataframe before calling inference methods.
        input_cols = self.input_cols
        unquoted_input_cols = identifier.get_equivalent_identifier_in_the_response_pandas_dataframe(self.input_cols)

        @pandas_udf(
            is_permanent=False,
            name=batch_inference_udf_name,
            packages=[{transform.predict_udf_deps}],
            replace=True,
            session=session
        )
        def vec_batch_infer(ds: PandasSeries[dict]) -> PandasSeries[dict]:
            import pandas as pd
            import numpy as np

            input_df = pd.io.json.json_normalize(ds)

            # pd.io.json.json_normalize() doesn't remove quotes around quoted identifiers like snowpakr_df.to_pandas().
            # But trained models have unquoted input column names saved in internal state if trained using snowpark_df
            # or quoted input column names saved in internal state if trained using pandas_df.
            # Model expects exact same columns names in the input df for predict call.

            input_df = input_df[input_cols] # Select input columns with quoted column names.
            if hasattr(estimator, "feature_names_in_"):
                missing_features = []
                for i, f in enumerate(getattr(estimator, "feature_names_in_")):
                    if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
                        missing_features.append(f)

                if len(missing_features) > 0:
                    raise ValueError(
                        "The feature names should match with those that were passed during fit.\n"
                        f"Features seen during fit call but not present in the input: {{missing_features}}\n"
                        f"Features in the input dataframe : {{input_cols}}\n"
                    )
                input_df.columns = getattr(estimator, "feature_names_in_")
            else:
                # Just rename the column names to unquoted identifiers.
                input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
            transformed_numpy_array = getattr(estimator, inference_method)(input_df)
            if (
                isinstance(transformed_numpy_array, list) 
                and len(transformed_numpy_array) > 0 
                and isinstance(transformed_numpy_array[0], np.ndarray)
            ):
                # In case of multioutput estimators, predict_proba(), decision_function(), etc., functions return
                # a list of ndarrays. We need to concatenate them.
                transformed_numpy_array = np.concatenate(transformed_numpy_array, axis=1)

            if len(transformed_numpy_array.shape) == 3:
                # VotingClassifier will return results of shape (n_classifiers, n_samples, n_classes)
                # when voting = "soft" and flatten_transform = False. We can't handle unflatten transforms,
                # so we ignore flatten_transform flag and flatten the results.
                transformed_numpy_array = np.hstack(transformed_numpy_array)

            if (
                len(transformed_numpy_array.shape) > 1 
                and transformed_numpy_array.shape[1] != len(expected_output_cols_list)
            ):
                # HeterogeneousEnsemble's transfrom method produce results with variying shapes
                # from (n_samples, n_estimators) to (n_samples, n_estimators * n_classes). 
                # It is hard to predict the response shape without using fragile introspection logic.
                # So, to avoid that we are packing the results into a dataframe of shape (n_samples, 1) with
                # each element being a list.
                if len(expected_output_cols_list) != 1:
                    raise TypeError("expected_output_cols_list must be same length as transformed array or "
                            "should be of length 1")
                series = pd.Series(transformed_numpy_array.tolist())
                transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
            else:
                transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
            return transformed_pandas_df.to_dict("records")

        batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{{safe_id}}".format(
            safe_id=self.id
        )

        pass_through_columns = self._get_pass_through_columns(dataset)
        # Run Transform
        query_from_df = str(dataset.queries["queries"][0])
        
        outer_select_list = pass_through_columns[:]
        inner_select_list = pass_through_columns[:]

        outer_select_list.extend([
                "{{object_name}}:{{column_name}}{{udf_datatype}} as {{column_name}}".format(
                    object_name=batch_inference_udf_name,
                    column_name=c,
                    udf_datatype=(f"::{{expected_output_cols_type}}" if expected_output_cols_type else "")
                )
                for c in expected_output_cols_list
        ])

        inner_select_list.extend([
            "{{udf_name}}(object_construct({{input_cols_dict}})) AS {{udf_name}}".format(
                udf_name=batch_inference_udf_name,
                input_cols_dict=", ".join([f"'{{c}}', {{c}}" for c in self.input_cols]),
            )
        ])

        sql = """WITH {{input_table_name}} AS ({{query}})
                    SELECT
                      {{outer_select_stmt}}
                    FROM (
                      SELECT
                        {{inner_select_stmt}}
                      FROM {{input_table_name}}
                    )
               """.format(
            input_table_name=batch_inference_table_name,
            query=query_from_df,
            outer_select_stmt=", ".join(outer_select_list),
            inner_select_stmt=", ".join(inner_select_list),
        )

        return session.sql(sql)

    def _sklearn_inference(
        self,
        dataset: pd.DataFrame,
        inference_method: str,
        expected_output_cols_list: List[str]
    ) -> pd.DataFrame:
        output_cols = expected_output_cols_list.copy()
        transformed_numpy_array = getattr(self._sklearn_object, inference_method)(
            dataset[self.input_cols]
        )
        if (
                isinstance(transformed_numpy_array, list)
                and len(transformed_numpy_array) > 0
                and isinstance(transformed_numpy_array[0], np.ndarray)
            ):
                # In case of multioutput estimators, predict_proba(), decision_function(), etc., functions return 
                # a list of ndarrays. We need to concatenate them.

                # First compute output column names
                if(len(output_cols) == len(transformed_numpy_array)):
                    actual_output_cols = []
                    for idx, np_arr in enumerate(transformed_numpy_array):
                        for i in range(1 if len(np_arr.shape) <= 1 else np_arr.shape[1]):
                            actual_output_cols.append(f"{{output_cols[idx]}}_{{i}}")
                    output_cols = actual_output_cols
                # Concatenate np arrays
                transformed_numpy_array = np.concatenate(transformed_numpy_array, axis=1)

        if len(transformed_numpy_array.shape) == 3:
            # VotingClassifier will return results of shape (n_classifiers, n_samples, n_classes)
            # when voting = "soft" and flatten_transform = False. We can't handle unflatten transforms,
            # so we ignore flatten_transform flag and flatten the results.
            transformed_numpy_array = np.hstack(transformed_numpy_array)

        if len(transformed_numpy_array.shape) == 1:
            transformed_numpy_array = np.reshape(transformed_numpy_array, (-1, 1))

        shape = transformed_numpy_array.shape
        if shape[1] != len(output_cols):
            if len(output_cols) != 1:
                    raise TypeError("expected_output_cols_list must be same length as transformed array or "
                            "should be of length 1 or should be of length number of label columns")
            actual_output_cols = []
            for i in range(shape[1]):
                actual_output_cols.append(f"{{output_cols[0]}}_{{i}}")
            output_cols = actual_output_cols

        if self._drop_input_cols:
            dataset = pd.DataFrame(data=transformed_numpy_array, columns=output_cols)
        else:
            dataset = dataset.copy()
            dataset[output_cols] = transformed_numpy_array
        return dataset

    @available_if(_original_estimator_has_callable("predict"))
    @telemetry.send_api_usage_telemetry(
        project=_PROJECT,
        subproject=_SUBPROJECT,
        custom_tags=dict([("autogen", True)]),
    )
    def predict(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[DataFrame, pd.DataFrame]:
        """Predict lable values for each example in the input dataset.

        Args:
            dataset: Input dataset.

        Returns:
            Transformed dataset.
        """
        if isinstance(dataset, DataFrame):
            output_df = self._batch_inference(
                dataset=dataset,
                inference_method="predict",
                expected_output_cols_list=self.output_cols,
                expected_output_cols_type="{transform.udf_datatype}",
            )
        elif isinstance(dataset, pd.DataFrame):
            output_df = self._sklearn_inference(
                dataset=dataset,
                inference_method="predict",
                expected_output_cols_list=self.output_cols,)
        else:
            raise TypeError(
                f"Unexpected dataset type: {{type(dataset)}}."
                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
            )

        return output_df

    @available_if(_original_estimator_has_callable("transform"))
    @telemetry.send_api_usage_telemetry(
        project=_PROJECT,
        subproject=_SUBPROJECT,
        custom_tags=dict([("autogen", True)]),
    )
    def transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[DataFrame, pd.DataFrame]:
        """Transform the dataset.

        Args:
            dataset: Input dataset.

        Returns:
            Transformed dataset.
        """
        if isinstance(dataset, DataFrame):
            expected_dtype = "{transform.udf_datatype}"
            if {transform._is_heterogeneous_ensemble}: # is child of _BaseHeterogeneousEnsemble
                # transform() method of HeterogeneousEnsemble estimators return responses of varying 
                # shapes from (n_samples, n_estimators) to (n_samples, n_estimators * n_classes) (and everything in between)
                # based on init param values. We will convert that to pandas dataframe of shape (n_samples, 1) with
                # each row containing a list of values.
                expected_dtype = "ARRAY"

            output_df = self._batch_inference(
                dataset=dataset,
                inference_method="transform",
                expected_output_cols_list=self.output_cols,
                expected_output_cols_type=expected_dtype,
            )
        elif isinstance(dataset, pd.DataFrame):
            output_df = self._sklearn_inference(
                dataset=dataset,
                inference_method="transform",
                expected_output_cols_list=self.output_cols,
            )
        else:
            raise TypeError(
                f"Unexpected dataset type: {{type(dataset)}}."
                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
            )

        return output_df

    def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
        """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
        Returns an empty list if current object is not a classifier or not yet fitted.
        """
        if getattr(self._sklearn_object, "classes_", None) is None:
            return []

        classes = self._sklearn_object.classes_
        if isinstance(classes, numpy.ndarray):
            return [f'{{output_cols_prefix}}{{c}}' for c in classes.tolist()]
        elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
            # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
            output_cols = []
            for i, cl in enumerate(classes):
                # For binary classification, there is only one output column for each class
                # ndarray as the two classes are complementary.
                if len(cl) == 2:
                    output_cols.append(f'{{output_cols_prefix}}_{{i}}_{{cl[0]}}')
                else:
                    output_cols.extend([
                        f'{{output_cols_prefix}}_{{i}}_{{c}}' for c in cl.tolist()
                    ])
            return output_cols
        return []

    @available_if(_original_estimator_has_callable("predict_proba"))
    @telemetry.send_api_usage_telemetry(
        project=_PROJECT,
        subproject=_SUBPROJECT,
        custom_tags=dict([("autogen", True)]),
    )
    def predict_proba(
        self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "predict_proba_"
    ) -> Union[DataFrame, pd.DataFrame]:
        """Compute probabilities of possible outcomes for samples in dataset.

        Args:
            dataset: Input dataset. Shape (n_samples, n_features).
            output_cols_prefix: Prefix for the response columns

        Returns:
            Output dataset with probability of the sample for each class in the model.
            n_classes of new columns with output_cols_prefix as column name prefix will be present in the response
            dataframe.
        """
        if isinstance(dataset, DataFrame):
            output_df = self._batch_inference(
                dataset=dataset,
                inference_method="predict_proba",
                expected_output_cols_list=self._get_output_column_names(output_cols_prefix),
                expected_output_cols_type="float"
            )
        elif isinstance(dataset, pd.DataFrame):
            output_df = self._sklearn_inference(
                dataset=dataset,
                inference_method="predict_proba",
                expected_output_cols_list=self._get_output_column_names(output_cols_prefix),
            )
        else:
            raise TypeError(
                f"Unexpected dataset type: {{type(dataset)}}."
                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
            )

        return output_df

    @available_if(_original_estimator_has_callable("predict_log_proba"))
    @telemetry.send_api_usage_telemetry(
        project=_PROJECT,
        subproject=_SUBPROJECT,
        custom_tags=dict([("autogen", True)]),
    )
    def predict_log_proba(
        self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "predict_log_proba_"
    ) -> Union[DataFrame, pd.DataFrame]:
        """Compute log probabilities of possible outcomes for samples in dataset.

        Args:
            dataset: Input dataset. Shape (n_samples, n_features).
            output_cols_prefix: Prefix for the response columns

        Returns:
            Output dataset with log probability of the sample for each class in the model.
            n_classes of new columns with output_cols_prefix as column name prefix will be prresent in the response
            dataframe.
        """
        if isinstance(dataset, DataFrame):
            output_df = self._batch_inference(
                dataset=dataset,
                inference_method="predict_log_proba",
                expected_output_cols_list=self._get_output_column_names(output_cols_prefix),
                expected_output_cols_type="float"
            )
        elif isinstance(dataset, pd.DataFrame):
            output_df = self._sklearn_inference(
                dataset=dataset,
                inference_method="predict_log_proba",
                expected_output_cols_list=self._get_output_column_names(output_cols_prefix),
            )
        else:
            raise TypeError(
                f"Unexpected dataset type: {{type(dataset)}}."
                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
            )

        return output_df

    @available_if(_original_estimator_has_callable("decision_function"))
    @telemetry.send_api_usage_telemetry(
        project=_PROJECT,
        subproject=_SUBPROJECT,
        custom_tags=dict([("autogen", True)]),
    )
    def decision_function(
        self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "decision_function_"
    ) -> Union[DataFrame, pd.DataFrame]:
        """Evaluate the decision function for the samples in dataset.

        Args:
            dataset: Input dataset. Shape (n_samples, n_features).
            output_cols_prefix: Prefix for the response columns

        Returns:
            Output dataset with results of the decision function for the samples in input dataset.
            n_classes of new columns with output_cols_prefix as column name prefix will be prresent in the response
            dataframe.
        """
        if isinstance(dataset, DataFrame):
            output_df = self._batch_inference(
                dataset=dataset,
                inference_method="decision_function",
                expected_output_cols_list=self._get_output_column_names(output_cols_prefix),
                expected_output_cols_type="float"
            )
        elif isinstance(dataset, pd.DataFrame):
            output_df = self._sklearn_inference(
                dataset=dataset,
                inference_method="decision_function",
                expected_output_cols_list=self._get_output_column_names(output_cols_prefix),
            )
        else:
            raise TypeError(
                f"Unexpected dataset type: {{type(dataset)}}."
                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
            )

        return output_df


    @available_if(_original_estimator_has_callable("score"))
    def score(self, dataset: Union[DataFrame, pd.DataFrame]) -> float:
        """{transform.fit_docstring}

        Args:
            dataset: Input dataset.

        Returns:
            Score.
        """
        self._infer_input_output_cols(dataset)
        if isinstance(dataset, pd.DataFrame):
            return self._score_sklearn(dataset)
        elif isinstance(dataset, DataFrame):
            return self._score_snowpark(dataset)
        else:
            raise TypeError(
                f"Unexpected dataset type: {{type(dataset)}}."
                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
            )


    def _score_sklearn(self, dataset: pd.DataFrame) -> float:
        argspec = inspect.getfullargspec(self._sklearn_object.score)
        if "X" in argspec.args:
            args = {{'X': dataset[self.input_cols]}}
        elif "X_test" in argspec.args:
            args = {{'X_test': dataset[self.input_cols]}}
        else:
            raise RuntimeError("Neither 'X' or 'X_test' exist in argument")
        
        if self.label_cols:
            label_arg_name = "Y" if "Y" in argspec.args else "y"
            args[label_arg_name] = dataset[self.label_cols].squeeze()

        if self.sample_weight_col is not None and "sample_weight" in argspec.args:
            args['sample_weight'] = dataset[self.sample_weight_col].squeeze()

        score = self._sklearn_object.score(**args)
        return score


    def _score_snowpark(self, dataset: DataFrame) -> float:
        # Specify input columns so column pruing will be enforced
        selected_cols = (
            self.input_cols + self.label_cols + [self.sample_weight_col]
            if self.sample_weight_col is not None
            else []
        )
        if len(selected_cols) > 0:
            dataset = dataset.select(selected_cols)

        # Extract query that generated the dataframe. We will need to pass it to score procedure.
        query = str(dataset.queries["queries"][0])

        # Create a temp file and dump the score to that file.
        local_score_file_name = get_temp_file_path()
        joblib.dump(self._sklearn_object, local_score_file_name)

        # Create temp stage to run score.
        score_stage_name = "SNOWML_SCORE_{{safe_id}}".format(safe_id=self.id)
        session = dataset._session
        stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {{score_stage_name}};"
        SqlResultValidator(
            session=session,
            query=stage_creation_query
        ).has_dimensions(
            expected_rows=1, expected_cols=1
        ).has_value_match(
            row_idx=0,
            col_idx=0,
            expected_value=f"Stage area {{score_stage_name}} successfully created."
        ).validate()

        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
        # Put locally serialized score on stage.
        session.file.put(local_score_file_name, stage_score_file_name, auto_compress=False, overwrite=True)

        score_sproc_name = "SNOWML_SCORE_{{safe_id}}".format(safe_id=self.id)
        @sproc(
            is_permanent=False,
            name=score_sproc_name,
            packages=["snowflake-snowpark-python", {transform.fit_sproc_deps}],
            replace=True,
            session=session)
        def score_wrapper_sproc(
            session: Session,
            sql_query: str,
            stage_score_file_name: str,
            input_cols: List[str],
            label_cols: List[str],
            sample_weight_col: Optional[str]
        ) -> float:
            import joblib
            import numpy as np
            import os
            import pandas
            import tempfile
            import inspect
            {transform.fit_sproc_imports}

            df = session.sql(sql_query).to_pandas()

            local_score_file = tempfile.NamedTemporaryFile(delete=True)
            local_score_file_name = local_score_file.name
            local_score_file.close()

            session.file.get(stage_score_file_name, local_score_file_name)
            estimator = joblib.load(os.path.join(local_score_file_name, os.listdir(local_score_file_name)[0]))
            argspec = inspect.getfullargspec(estimator.score)
            if "X" in argspec.args:
                args = {{'X': df[input_cols]}}
            elif "X_test" in argspec.args:
                args = {{'X_test': df[input_cols]}}
            else:
                raise RuntimeError("Neither 'X' or 'X_test' exist in argument")
            
            if label_cols:
                label_arg_name = "Y" if "Y" in argspec.args else "y"
                args[label_arg_name] = df[label_cols].squeeze()

            if sample_weight_col is not None and "sample_weight" in argspec.args:
                args['sample_weight'] = df[sample_weight_col].squeeze()

            result = estimator.score(**args)
            return result
        
        # Call score sproc
        statement_params = telemetry.get_function_usage_statement_params(
            project=_PROJECT,
            subproject=_SUBPROJECT,
            function_name=telemetry.get_statement_params_full_func_name(
                inspect.currentframe(), self.__class__.__name__
            ),
            api_calls=[Session.call],
            custom_tags=dict([("autogen", True)]),
        )
        score = session.call(
            score_sproc_name,
            query,
            stage_score_file_name,
            identifier.get_equivalent_identifier_in_the_response_pandas_dataframe(self.input_cols),
            identifier.get_equivalent_identifier_in_the_response_pandas_dataframe(self.label_cols),
            identifier.get_equivalent_identifier_in_the_response_pandas_dataframe(self.sample_weight_col),
            statement_params=statement_params,
        )

        cleanup_temp_files([local_score_file_name])

        return score
